# Personal-AI-Twin
Personal AI/LLM: scrapes my posts, streams them through MongoDB CDCâ†’RabbitMQâ†’Bytewax, embeds into Qdrant, fineâ€‘tunes an 8B model (LoRA/QLoRA) on AWSâ€¯SageMaker, deploys a RAG API + Gradio UI that writes like me. Terraform + Docker + CI/CD, Comet/Opik for tracking, subâ€‘second responses, &lt;$0.05/1k queries. Plugâ€‘andâ€‘play.
# ğŸ§‘â€ğŸ’» LLMÂ Twin â€“â€¯My Personal AIÂ Replica

---

## Table of Contents

1. [Project Goals](#project-goals)
2. [Architecture](#architecture)
3. [QuickÂ Start](#quick-start)
4. [Detailed Pipelines](#detailed-pipelines)
5. [TechÂ Stack](#tech-stack)
6. [Project Structure](#project-structure)
7. [Cost Footprint](#cost-footprint)
8. [Contributing](#contributing)
9. [License](#license)

---

## Project Goals

* **Create an "LLM twin"** that mimics my writing voice across blogs and code comments.
* Showcase **productionâ€‘grade LLMOps**: reproducible data pipelines, experiment tracking, model registry, prompt monitoring, and automated deployment.
* Keep costs <â€¯\$10 by relying on free tiers + spot instances.

## Architecture

<p align="center">
  <img src="docs/architecture.png" alt="System architecture diagram" width="700"/>
</p>

### Highâ€‘level Flow

1. **DataÂ CollectionÂ Pipeline** â€“Â Serverless crawlers pull content from Medium, Substack, LinkedIn, and GitHub into MongoDB; changes are emitted via CDC to RabbitMQ.
2. **FeatureÂ Pipeline** â€“Â Bytewax ingests the queue, cleans & chunks text, generates embeddings, and upserts vectors to Qdrant (optionally Redis) in realâ€‘time.
3. **TrainingÂ Pipeline** â€“Â Autoâ€‘builds an instruction dataset, fineâ€‘tunes an 8â€¯B model with LoRA/QLoRA on SageMaker, logs runs to CometÂ ML, evaluates with Opik, and versions artifacts on HuggingÂ Face Hub.
4. **InferenceÂ Pipeline** â€“Â Loads & quantizes the accepted model, wraps a RAG layer with hybrid BM25Â +Â vector search, exposes a REST API on SageMaker endpoints, monitors prompts/answers, and surfaces a Gradio UI.

## QuickÂ Start

```bash
# Clone & setup
git clone https://github.com/<yourâ€‘handle>/llmâ€‘twin.git
cd llmâ€‘twin
cp .env.example .env   # fill in secrets
make bootstrap          # installs deps & preâ€‘commit hooks

# 1ï¸âƒ£Â Kick off data crawl (runs as AWSÂ Lambda locally)
make crawl

# 2ï¸âƒ£Â Start streaming feature pipeline
make stream

# 3ï¸âƒ£Â Launch fineâ€‘tuning job on SageMaker (uses spot instances)
make train

# 4ï¸âƒ£Â Deploy inference API + UI
make deploy
```

> **Note:** Detailed, stepâ€‘byâ€‘step instructions live in [`docs/INSTALL_AND_USAGE.md`](docs/INSTALL_AND_USAGE.md).

## Detailed Pipelines

| Stage                 | Highlights                                                          |
| --------------------- | ------------------------------------------------------------------- |
| **Dataâ€¯â†’â€¯Queue**      | ETL in Python, packaged as Lambda; MongoDBâ€¯â†’â€¯RabbitMQ CDC connector |
| **Queueâ€¯â†’â€¯VectorÂ DB** | Bytewax stream; Superlinked refactor cuts 70â€¯% of boilerplate       |
| **Fineâ€‘tune**         | 8â€¯B model, LoRA rankâ€¯=â€¯64, 4â€‘bit QLoRA; tracked with CometÂ Â         |
| **Evaluation**        | Automatic perplexity & style metrics via Opik; manual spot checks   |
| **Serving**           | Textâ€‘generationâ€‘inference (TGI) container on SageMaker, autoscaling |
| **Monitoring**        | Prompt/response logging, latency & cost dashboards                  |

## TechÂ Stack

| Layer             | Tools                                                |
| ----------------- | ---------------------------------------------------- |
| **Lang/Runtime**  | PythonÂ 3.11, Docker, Makefile                        |
| **Data**          | MongoDB, RabbitMQ, Bytewax                           |
| **VectorÂ Store**  | Qdrant, RedisÂ (VS)                                   |
| **Modeling**      | HuggingÂ FaceÂ Transformers, LoRA/QLoRA, AWSÂ SageMaker |
| **Observability** | CometÂ ML, Opik, CloudWatch                           |
| **Infraâ€‘asâ€‘Code** | Terraform, GitHubÂ Actions CI/CD                      |
| **UI**            | Gradio                                               |

## Project Structure

```text
llmâ€‘twin/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_crawling/
â”‚   â”œâ”€â”€ data_cdc/
â”‚   â”œâ”€â”€ feature_pipeline/
â”‚   â”œâ”€â”€ training_pipeline/
â”‚   â””â”€â”€ inference_pipeline/
â”œâ”€â”€ terraform/           # IaC for AWS resources
â”œâ”€â”€ docs/                # Diagrams & guides
â”œâ”€â”€ .env.example
â”œâ”€â”€ Makefile
â””â”€â”€ pyproject.toml
```

## Cost Footprint

| Provider          | Purpose                     | Est.Â Cost |
| ----------------- | --------------------------- | --------- |
| OpenAI (optional) | GPTâ€‘4 eval prompts          | \~Â \$1    |
| AWSÂ SageMaker     | Fineâ€‘tune + endpoint (spot) | <Â \$10    |
| All others        | Free tier                   |           |

## Contributing

Found a bug or have an idea? PRs & issues welcome! See [`CONTRIBUTING.md`](CONTRIBUTING.md).

## License

MIT â€“Â free to fork, tweak, and deploy. Just keep the license & attribution.

---

> *Inspired by the excellent LLM Twin framework, reâ€‘implemented & customised for my own workflow.*
